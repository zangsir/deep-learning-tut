{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An exmaple of entity embeddings on the Kaggle's https://www.kaggle.com/c/porto-seguro-safe-driver-prediction challenge  \n",
    "\n",
    "###### Most of the code has been taken from Joe Eddy's kernel available here https://www.kaggle.com/aquatic/entity-embedding-neural-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing libraries: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time  = int(time.time())\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading and shuffling training file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As classes are highly imbalanced, using only a part of the larger-class data while training : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21694\n",
      "573518\n"
     ]
    }
   ],
   "source": [
    "df_train_cat0 = df_train.loc[ df_train['target'] == 0 ]\n",
    "df_train_cat1 = df_train.loc[ df_train['target'] == 1 ]\n",
    "print len(df_train_cat1)\n",
    "print len(df_train_cat0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train_cat0[:40000].append(df_train_cat1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    40000\n",
       "1    21694\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shuffling again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Taking out labels column and droping unnecesary columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_labels = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(\"id\", 1)\n",
    "df_train = df_train.drop(\"target\", 1)\n",
    "all_cols = df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Breaking into 3 columns type, binary, categorical and other ( continous):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoical_vars = [ ]\n",
    "continous_vars = [ ]\n",
    "categoical_binary_vars = [ ]\n",
    "\n",
    "for col in all_cols:\n",
    "    #if \"bin\" in col :\n",
    "    #    categoical_binary_vars.append(col)\n",
    "    if \"cat\" in col :\n",
    "        categoical_vars.append(col)\n",
    "    else:\n",
    "        continous_vars.append(col )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('categorical binary vars: ', 0)\n",
      "('categorical non binary vars: ', 14)\n",
      "('continues vars: ', 43)\n"
     ]
    }
   ],
   "source": [
    "print (\"categorical binary vars: \", len(categoical_binary_vars))\n",
    "print (\"categorical non binary vars: \", len(categoical_vars))\n",
    "print (\"continues vars: \", len(continous_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displaying number of unique vals in each categorical column: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ps_ind_02_cat', 5)\n",
      "('ps_ind_04_cat', 3)\n",
      "('ps_ind_05_cat', 8)\n",
      "('ps_car_01_cat', 13)\n",
      "('ps_car_02_cat', 2)\n",
      "('ps_car_03_cat', 3)\n",
      "('ps_car_04_cat', 10)\n",
      "('ps_car_05_cat', 3)\n",
      "('ps_car_06_cat', 18)\n",
      "('ps_car_07_cat', 3)\n",
      "('ps_car_08_cat', 2)\n",
      "('ps_car_09_cat', 6)\n",
      "('ps_car_10_cat', 3)\n",
      "('ps_car_11_cat', 104)\n"
     ]
    }
   ],
   "source": [
    "for cat_var in categoical_vars:\n",
    "    print (cat_var, df_train[cat_var].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making train and val set : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_train[:40000]\n",
    "df_val = df_train[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train  = Y_labels[:40000]\n",
    "Y_val  = Y_labels[40000: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW LETS WORK WITH MODEL BUILDING\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_cols = [i for i in all_cols if i not in categoical_vars ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For purpose of understanding lets only consider 2 categorical columns and one unique columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoical_vars = categoical_vars[:2]\n",
    "other_cols = other_cols[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding th eembedding size thing: \n",
    " \n",
    "#say there is a string [\"I am a man\"]\n",
    "#coresponding input will be [\"12 56 89 74\"]\n",
    "#Now for each word will converted to some vector of lenght embedding size (say 2 here )\n",
    "#coresponding output will be  [[0.2 0.3], [0.12, 0.25], [78, 45]]\n",
    "#And for multiple ips, multiple ops \n",
    "\n",
    "  # the model will take as input an integer matrix of size (batch, input_length).\n",
    "  # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "  # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "  # now model.output_shape == (None, 10, 64), where None is the batch dimension, 10 is embedding size and 64 is input length \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(X_train ) : \n",
    "\n",
    "    input_list_train = []\n",
    "    \n",
    "    \n",
    "    #the cols to be embedded: rescaling to range [0, # values)\n",
    "    for c in categoical_vars :\n",
    "        \n",
    "        \"\"\"\n",
    "        vals = np.asarray(X_train[c].tolist() )\n",
    "        input_list_train.append( np.asarray( vals ))\n",
    "        this fails as keras Expect 0,1,2,3.. as cat and not 1,2,3,5 if there are 4 categories.\n",
    "        Using below method instead from https://stackoverflow.com/a/45988584 \n",
    "        \n",
    "        \"\"\"\n",
    "        vals = np.asarray(X_train[c].tolist())\n",
    "        vals = pd.factorize( vals )[0]\n",
    "        input_list_train.append( np.asarray(vals)  )\n",
    "        \"\"\"\n",
    "        This below was the original method used in the code by INSERT NAME HERE. But I found the above implemntation much simpler to understand.\n",
    "        raw_vals = np.unique(X_train[c])\n",
    "        val_map = {}\n",
    "        for i in range(len(raw_vals)):\n",
    "            val_map[raw_vals[i]] = i       \n",
    "        input_list_train.append(X_train[c].map(val_map).values)\n",
    "        \"\"\"\n",
    "    #the rest of the columns\n",
    "    input_list_train.append(X_train[other_cols].values)\n",
    "    return input_list_train \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr_modified = preproc( df_tr ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = preproc( df_val )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As expected, the proceesed training DataFrame has 3 elements becasue we had 2 categorical columns and one for other continous columns : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_tr_modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The lenght of each list in the processed dataframe is same as the number of training rows we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000\n",
      "40000\n",
      "40000\n"
     ]
    }
   ],
   "source": [
    "print len(df_tr_modified[0])\n",
    "print len(df_tr_modified[1])\n",
    "print len(df_tr_modified[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 1]\n",
      "0\n",
      "1\n",
      "[0 0 0 ... 0 0 1]\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print df_tr_modified[0]\n",
    "print df_tr_modified[0][0]\n",
    "print df_tr_modified[0].ndim\n",
    "\n",
    "print df_tr_modified[1]\n",
    "print df_tr_modified[1][0]\n",
    "print df_tr_modified[1].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6 4 0]\n",
      " [1 8 1]\n",
      " [0 4 0]\n",
      " ...\n",
      " [0 2 0]\n",
      " [2 6 1]\n",
      " [3 3 0]]\n",
      "---\n",
      "[6 4 0]\n",
      "---\n",
      "6\n",
      "---\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print df_tr_modified[2]\n",
    "print \"---\"\n",
    "print df_tr_modified[2][0]\n",
    "print \"---\"\n",
    "print df_tr_modified[2][0][0]\n",
    "print \"---\"\n",
    "print df_tr_modified[2].ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr_modified[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  The below code adds a embedding network for each of the catgeoriacal variable. The embedding size is set as according to the rule: \n",
    "```\n",
    "Embedding size  = min( no-of-unique-cat/2 , 50 )\n",
    "```\n",
    "##### Each  model is appending to a list named models defined intialised in below cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------\n",
      "for categoical column  ps_ind_02_cat\n",
      "number of unique cat 5\n",
      "embedding_size set as  2\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1, 2)              12        \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "------------------------------------------------------------------\n",
      "for categoical column  ps_ind_04_cat\n",
      "number of unique cat 3\n",
      "embedding_size set as  1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1, 1)              4         \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 4\n",
      "Trainable params: 4\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for categoical_var in categoical_vars :\n",
    "    print \"------------------------------------------------------------------\"\n",
    "    print \"for categoical column \", categoical_var     \n",
    "    model = Sequential()\n",
    "    no_of_unique_cat  = df_train[categoical_var].nunique()\n",
    "    print \"number of unique cat\",no_of_unique_cat\n",
    "    embedding_size = min(np.ceil((no_of_unique_cat)/2), 50 )\n",
    "    embedding_size = int(embedding_size)\n",
    "    print \"embedding_size set as \", embedding_size\n",
    "    model.add(  Embedding( no_of_unique_cat+1, embedding_size, input_length = 1 ) )\n",
    "    \n",
    "    model.add(Reshape(target_shape=( [embedding_size] )))\n",
    "\n",
    "    \n",
    "    print model.summary() \n",
    "    \n",
    "    models.append( model )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Once the categorical columns are made, we add another single model for all the continous variables and add it to the models list ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 16)                64        \n",
      "=================================================================\n",
      "Total params: 64\n",
      "Trainable params: 64\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_rest = Sequential()\n",
    "model_rest.add(Dense(16, input_shape = [3] ))\n",
    "#model_rest.add(Reshape(target_shape=([16] )))\n",
    "model_rest.summary() \n",
    "models.append(model_rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our models list will contain N-cat+1 models. ( N-cat models for each of the categorical columns and one model for all other columns.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.models.Sequential at 0x7f59237860d0>,\n",
       " <keras.models.Sequential at 0x7f5923786250>,\n",
       " <keras.models.Sequential at 0x7f592375e690>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally we merge all the models present into a  single model. Concat places the model one after the other "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ms/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "full_model = Sequential()\n",
    "full_model.add(Merge(models, mode='concat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The ouptut szie from the merged model is 19 which is actually ( 2 + 1 + 16 ).  (The size e1 + e2 and the 16 outputs from dense layer. ) : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 19)                0         \n",
      "=================================================================\n",
      "Total params: 80\n",
      "Trainable params: 80\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the below msg is good for understanding what happens when we merge using the `add` mode. The add mode does a element  wise addtion while concat appends it one after another\n",
    "\n",
    "```\n",
    "Code:\n",
    "full_model = Sequential()\n",
    "full_model.add(Merge(models, mode='sum'))\n",
    "\n",
    "O/p: \n",
    "ValueError: Only layers of same output shape can be merged using sum mode. Layer shapes: [(None, 2), (None, 1), (None, 16)]\n",
    "Here the lengths of 3 vectors to be added are 2,1, 16 thus add mode concatenation cannot be done \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now are input layers and embedding layers are done. We can build on those as any other keras sequential model.\n",
    "##### Adding a few layers below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.add(Dense(1024))\n",
    "full_model.add(Activation('relu'))\n",
    "full_model.add(Dense(512))\n",
    "full_model.add(Activation('relu'))\n",
    "full_model.add(Dense(256))\n",
    "full_model.add(Activation('sigmoid'))\n",
    "\n",
    "full_model.add(Dense(2))\n",
    "full_model.add(Activation('sigmoid'))\n",
    "full_model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 19)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1024)              20480     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              3072      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 1,336,916\n",
      "Trainable params: 1,336,916\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "Y_train_cat = to_categorical(Y_train.tolist() )\n",
    "Y_val_cat = to_categorical(Y_val.tolist()  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40000/40000 [==============================] - 31s 764us/step - loss: 0.6507 - acc: 0.6494\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f592018dc10>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_model.fit( df_tr_modified, Y_train_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
